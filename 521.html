<!-- saved from url=(0062)http://www.genome.duke.edu/labs/engelhardt/courses/sta521.html -->
<html><head><meta http-equiv="Content-Type" content="text/html;
                                                     charset=UTF-8"><title>STA521 Predictive Modeling and Statistical Learning, Fall 2020</title>
    <style type="text/css">
    </style>
</head>

<body bgcolor="#FFFFFF" text="#000000" link="#6633FF" vlink="#007f0f">

<h2 align="center">STA521: Predictive Modeling and Statistical Learning: Fall 2020</h2>
  
<table align="center">
<table align="center"> 

<tbody><td>Prof:</td><td><a href="http://www.stat.duke.edu/~sayan/">Sayan Mukherjee</a></td>
                 <td><a href="mailto:sayan@stat.duke.edu">
                 <i>sayan</i></a><i>@stat.duke.edu</i></td><td></td>
                 <td>OH: T/W/Th 1-2pm</td><td>https://duke.zoom.us/j/6247790803</td></tr>                 
<tr><td>TA:</td><td> Bo Liu  </td><td> bo.liu1997@duke.edu</td><td>OH:
    TBD
    </td><td></td><td></td></tr> 
<tr><td>Class:</td><td>T/TH 10:15-11:30am</td><td></td><td></td><td></td><td> </td></tr> 
<tr><td>Lab:</td><td>M 10:15-11:30am</td><td></td><td></td><td></td><td> </td></tr>   
</tbody></table>



<hr>
<h3 align="center">Description</h3>
An introduction to statistical learning methods for prediction and inference. This course introduces students to concepts and techniques of Classical and Bayesian approaches for modern regression and predictive modelling. The course will blend theory and application using a range of examples. Topics include exploratory data analysis and visualization, linear and generalized linear models, model selection, penalized estimation and shrinkage methods including Lasso, ridge regression and Bayesian regression, regression and classification based on decision trees, Bayesian Model Averaging and ensemble methods, and time permitting, robust estimation, smoothing splines, support vector machines, neural nets or other advanced topics. The R programming language and applications are used throughout. Corequisite: Statistical Science 323D or 523L and Statistical Science 360, 601, or 602L.HMMs,  MCMC. Emphasis is on applying these techniques to real data in a variety of application areas.
<p>
All students should be comfortable with linear/matrix algebra and mathematical statistics at the level of STA 611 (Statistical Inference - Casella and Berger is an excellent resource) and familiar with the R programming language and should be familiar with linear regression. Students should be familiar with Bayesian statistics either by taking the introduction to Bayesian inference STA 360/601/602 or currently co-registered in the course. Please see me if you have questions about the pre-requisites/background.
<p>


<hr>
<h3 align="center">Texts</h3>
The course will follow the texts and notes developed by Merlise Clyde,  <a href="https://www2.stat.duke.edu/courses/Fall19/sta521/#projects"> Fall 2019</a>

Some texts that will be used throught the semester
</p><ol>
<li> An Introduction to Statistical Learning: with Applications in R. <a href="http://getitatduke.library.duke.edu/?sid=sersol&SS_jc=TC0000935811&title=An%20Introduction%20to%20Statistical%20Learning%20with%20Applications%20in%20R"> Freely available as an eBook</a>  
</li><li>Elements of Statistical Learning. <a href="https://web.stanford.edu/~hastie/ElemStatLearn/">Freely available as an eBook</a>
</li><li>Applied Linear Regression. <a href="http://search.library.duke.edu/search?id=DUKE005781635" target="_blank">Freely available from get it @ Duke</a>
</li><li>Data Analysis Using Regression and  Multilevel/Hierarchical Models. <a href="https://www.amazon.com/Analysis-Regression-Multilevel-Hierarchical-Models/dp/052168689X/ref=sr_1_1_twi_pap_2?s=books&amp;ie=UTF8&amp;qid=1483554410&amp;sr=1-1&amp;keywords=9780521686891"> From Amazon</a> 
</li><li>A First Course in Bayesian Statistical Methods. <a href="http://getitatduke.library.duke.edu/?sid=sersol&amp;SS_jc=TC0000296463&amp;title=A%20First%20Course%20in%20Bayesian%20Statistical%20Methods">Freely avaialable as an eBook via Duke library</a>
  </li>
</ol>
<p>  

<p>Linear and matrix algebra will be used extensively, for a review or brush-up look at <a href="https://www2.stat.duke.edu/courses/Fall19/sta521/#resources"> these resources</a>.
 
  
  
<hr>
<h3 align="center">Computation</h3>

<p>We will use R as a programming language for computation and data analysis, with the use existing packages written in R to support the course. All students will have access to RStudio/R on a server within the department and support during the labs. You are free to run RStudio/R on your personal laptop or desktop. </p> 

<p>There will also be some use of JAGS in this course for heirarchical modeling.</p>
  
<p>You will also need to be able to use git/github for you homeworks and downloading/adapting code.</p>  

<p>Beyond the resources on this page, feel free to look at <a href="https://www2.stat.duke.edu/courses/Fall19/sta521/#resources"> these resources</a>.


<p><a href="http://www.r-project.org" target="_blank">R</a> is a statistical programming language that is especially powerful for data exploration, visualization, and statistical analysis. To interact with R (initially), we will primarily be using <a href="http://www.rstudio.com/" target="_blank">RStudio</a>, an interactive development environment (IDE).</p>

<p>We will mostly be using a browser based version of RStudio on a remote server but you can also install a local version of the <a href="http://www.rstudio.com/ide/download/desktop" target="_blank">RStudio IDE</a>.</p>
  
<p>Books &amp; Resources for learning R</p>

<ul>
<li><a href="https://www.codeschool.com/courses/try-r" target="_blank">Codeschool - Try R</a>  A brief R tutorial, in case you would like to have another avenue by which to get introduced to R.</li>
<li><a href="http://r4ds.had.co.nz" target="_blank">R for Data Science</a> - Grolemund, Wickham
O&rsquo;Reilly, 1st edition, 2016 (ISBN:)</li>
<li><a href="http://www.amazon.com/ggplot2-Elegant-Graphics-Data-Analysis/dp/0387981403/ref=pd_sim_14_3?ie=UTF8&amp;refRID=0PWK3YY76N4T5G8KQRFF" target="_blank">ggplot2: Elegant Graphics for Data Analysis Use R! Buy it at
Amazon</a></li>
<li><a href="http://adv-r.had.co.nz" target="_blank">Advanced R</a> - H. Wickham
Chapman and Hall/CRC, 1st edition, 2014 (ISBN: 978-1466586963)</li>
<li><a href="http://r-pkgs.had.co.nz" target="_blank">R Packages</a> - H Wickham
O&rsquo;Reilly, 1st edition, 2015 (ISBN: 978-1491910597)
*<a href="https://www.r-project.org" target="_blank">The R Project for Statistical Computing</a></li>
<li><a href="http://archive.linux.duke.edu/cran/" target="_blank">R Downloads</a> Duke Mirror with Linux, Mac, Windows</li>
<li><a href="https://www.rstudio.com" target="_blank">Rstudio</a> Easy user interface for R/R Markdown and more for Linux, Mac and Windows  <a href="https://www.rstudio.com/products/RStudio/" target="_blank">Download</a></li>
<li><a href="https://cran.r-project.org/doc/manuals/R-intro.pdf" target="_blank">An Introduction to R (pdf)</a>  <a href="https://cran.r-project.org/doc/manuals/R-intro.html" target="_blank">(html version)</a> the most up-to-date official R intro</li>
<li><a href="http://www.twotorials.com/" target="_blank">twotorials</a>: how to do stuff in r. two minutes or less.</li>
</ul>



<p>JAGS is Just Another Gibbs Sampler.  It is a program for analysis of Bayesian hierarchical models using Markov Chain Monte Carlo (MCMC)
simulation  not wholly unlike BUGS. The name is a misnomer as JAGS implements more than just Gibbs Samplers.
JAGS was written with three aims in mind:</p>

<ul>
<li>To have a cross-platform engine for the BUGS language</li>
<li>To be extensible, allowing users to write their own functions, distributions and samplers.</li>
<li>To be a plaftorm for experimentation with ideas in Bayesian modelling</li>
</ul>

<p>Resources for JAGS:</p>

<ul>
<li><a href="http://mcmc-jags.sourceforge.net" target="_blank">JAGS website on Sourceforge for Downloads, Manuals, etc</a></li>
<li><a href="https://martynplummer.wordpress.com" target="_blank">JAGS News</a>  Martin Plummer&rsquo;s</li>
<li>site for all things JAGS</li>
<li><a href="https://cran.r-project.org/web/packages/rjags/" target="_blank">rjags on CRAN</a></li>
</ul>



<p>Git is a state-of-the-art version control system. It lets you track who made changes to what when and has options for easily updating a shared or public version of your code on <a href="https://github.com/" target="_blank">github</a>.</p>

<ul>
<li><p>OSX - install Git for Mac by downloading and running <a href="http://git-scm.com/downloads" target="_blank">the installer</a> or install <a href="http://brew.sh/" target="_blank">homebrew</a> and use it to install git via <code>brew install git</code>.</p></li>

<li><p>Unix/Linux - you should be able to install git via your prefered package manager (if it is not already installed).</p></li>

<li><p>Windows - install Git for Windows by download and running the git for windows <a href="http://msysgit.github.io/" target="_blank">installer</a>. This will provide you with git, the bash shell, and ssh in windows.</p></li>

<li><p><a href="http://www2.stat.duke.edu/~cr173/Sta523_Fa16/" target="_blank">Screencasts from STA523</a> will be helpful in refreshing/learning how
to use git and github</p></li>
</ul>  
  
  
 

There is a <a href="https://piazza.com/duke/spring2020/sta561">Piazza</a> course
discussion page. Please direct questions about homeworks and other
matters to that page. Otherwise, you can email the instructors (TAs
and professor). Note that we are more likely to
respond to the Piazza questions than to the email, and your classmates
may respond too, so that is a good place to start.



<hr>
<h3 align="center">Course schedule</h3>




This schedule is <i>tentative</i>, and will almost surely be modified. Reload your browser for the current version.
</p><p>


<a name="syl"></a>
<h3 align="center"><a name="syl">Schedule</a></h3>
<p>

</p><ol>
  <li> (Aug 18th) Course introduction: <a href="sayanmuk.github.io/Lecture 1.htm">Lecture</a> <br>
  Course overview and introduction to statistical learning and predictive inference. Read Chapter 1 of ISLR</li>
<!--<ul>
<li> Optional: (video) Christopher Bishop <a href="http://scpro.streamuk.com/uk/player/Default.aspx?wid=7739">Embracing Uncertainty: The New Machine Intelligence</a></li>
<li> Optional: (video) Sam
  Roweis <a href="http://videolectures.net/mlss06tw_roweis_mlpgm/">Machine
    Learning, Probability and Graphical Models, Part 1</a></li>
<li> Optional: (video) Mikaela
  Keller <a href="http://videolectures.net/bootcamp07_keller_bss/">
  Basics of probability and statistics for statistical
  learning</a></li>
<li> Optional: Alan
  Turing <a href="http://www.csee.umbc.edu/courses/471/papers/turing.pdf">Computing  Machinery and Intelligence</a></li>
</ul></li>
</ul>-->
  <br>
 <li> (Aug 20th) Modelling Overview and Review of Regression: <a href="https://www2.stat.duke.edu/courses/Fall19/sta521/lectures/Lec-2-Models/Models.pdf">Lecture</a> <br>
  Justification of models and review of linear regression. Read Chapter 2-3 of Elements of Statistical Learning or Chapters 2-3 ISLR</li>
  <br>Homework 1: Complete the problems in the github repo for <a href="https://github.com/sta521-f19/HW1"> HW 1</a>
<br>1) Login to saxon and Rstudio <a href="https://saxon.stat.duke.edu:8787"> https://saxon.stat.duke.edu:8787</a> or use your local computer.
<br>2) Create an a new project in RStudio (see lab1 instructions) and clone the repo using the link <a href="http://github.com/sta521-f19/HW1.git">http://github.com/sta521-f19/HW1.git</a>
<br>3) Work the problems in HW1.Rmd (applied and theory)
<br>4) Upload your Rmd file and a pdf to Sakai under assignments by Aug 28 10 am.
  </li>
<!--<ul>
<li> Optional: (video) Christopher Bishop <a href="http://scpro.streamuk.com/uk/player/Default.aspx?wid=7739">Embracing Uncertainty: The New Machine Intelligence</a></li>
<li> Optional: (video) Sam
  Roweis <a href="http://videolectures.net/mlss06tw_roweis_mlpgm/">Machine
    Learning, Probability and Graphical Models, Part 1</a></li>
<li> Optional: (video) Mikaela
  Keller <a href="http://videolectures.net/bootcamp07_keller_bss/">
  Basics of probability and statistics for statistical
  learning</a></li>
<li> Optional: Alan
  Turing <a href="http://www.csee.umbc.edu/courses/471/papers/turing.pdf">Computing  Machinery and Intelligence</a></li>
</ul></li>
</ul>-->
<br>
 <br>  
<br>
(Aug 24) Lab 1: <a href="https://github.com/STA521-F19/Lab1">RStudio, Git, & Reproducible EDA using Rmarkdown</a> 
<br>
<br>
 <li> (Aug 25th) Diagnostics: <a href="https://www2.stat.duke.edu/courses/Fall19/sta521/lectures/Lec-3-Diagnostics/Diagnostics.pdf">Lecture</a> <br>
  Regression diagnostics for linear regression. Read ALR Ch 9 or ALR Chapter 9 (4th Edition) and Computing Primer for ALR
</li>
<!--<ul>
<li> Optional: (video) Christopher Bishop <a href="http://scpro.streamuk.com/uk/player/Default.aspx?wid=7739">Embracing Uncertainty: The New Machine Intelligence</a></li>
<li> Optional: (video) Sam
  Roweis <a href="http://videolectures.net/mlss06tw_roweis_mlpgm/">Machine
    Learning, Probability and Graphical Models, Part 1</a></li>
<li> Optional: (video) Mikaela
  Keller <a href="http://videolectures.net/bootcamp07_keller_bss/">
  Basics of probability and statistics for statistical
  learning</a></li>
<li> Optional: Alan
  Turing <a href="http://www.csee.umbc.edu/courses/471/papers/turing.pdf">Computing  Machinery and Intelligence</a></li>
</ul></li>
</ul>-->
  <br>


 <li> (Aug 25th) Modelling Overview and Review of Regression:
<a href="https://www2.stat.duke.edu/courses/Fall19/sta521/publication/lec-2-models/">Lecture</a></li>
<ul>
<li> Optional: Norman R. Draper and R. Craig van Nostrand <a href="http://www.stat.duke.edu/~sayan/561/2015/Ridge.pdf">Ridge regression</a></li>
<li> Optional: Elements of Statistical
  Learning <a href="http://statweb.stanford.edu/~tibs/ElemStatLearn/printings/ESLII_print10.pdf">Pages
    61-67</a></li>
<li> Optional: (video) Michael Jordan <a href="http://videolectures.net/mlss09uk_jordan_bfway/">Bayesian or Frequentist: Which Are You?</a></li>
<li> Optional: Proof that leave-k-out is
  unbiased <a href="http://www.stat.duke.edu/~sayan/561/2015/class20.pdf">Lecture
  notes based on: A. Luntz and V. Brailovsky. Technicheskaya Kibernetica, 3, 1969.</a></li>
</ul>
<BR>  
</li><li> (Jan 22nd) Bayesian motivation for proceduralist
  approach: <a href="http://www.stat.duke.edu/~sayan/561/2019/lec3.pdf">Lecture</a></li>
<ul>
<li> Optional: (video) Alex Smola <a href="http://videolectures.net/mlss06au_smola_ef/">Exponential  Families</a></li>
<li> Strongly suggested: <a href="http://www.stat.duke.edu/~sayan/561/2015/stat_ml.pdf">Useful properties of the multivariate normal in notes</a></li>
<li> Optional*: Persi Diaconis and Donald Ylvisaker <a href="http://statweb.stanford.edu/~cgates/PERSI/papers/conjprior.pdf">Conjugate priors for exponential families </a></li>
</ul>
<BR>
</li>
<li> (Jan 24th) Bayesian linear regression:
<a href="http://www.stat.duke.edu/~sayan/561/2019/lec4.pdf">Lecture</a> </li>
<ul>
<li> Optional: (video) <a href="https://vimeo.com/14553953">LISA Short
    Course: Regression Using Bayesian Statistics in R</a> </li>
<li> Strongly
  suggested: <a href="http://www.stat.duke.edu/~sayan/561/2015/stat_ml.pdf">
    Review of Functional analysis in notes</a></li>
  </ul></ul>
  </ul>
<BR>
<b> <a href="http://www.stat.duke.edu/~sayan/561/2019/HW1.pdf">HW
    1  </a> <a href="http://www.stat.duke.edu/~sayan/561/2019/Advertisement.csv">Data
    for HW
    1</a> <a href="http://www.stat.duke.edu/~sayan/561/2019/HW1sol.pdf">
    HW
    1 solutions  </a>  </b>
<BR>
<BR>
<li> (Jan 29th) Regularized logistic
  regression:<a href="http://www.stat.duke.edu/~sayan/561/2019/lec8.pdf">
  Lecture</a> and <a href="http://www.stat.duke.edu/~sayan/561/2019/SVM.pdf">  
    Support Vector Machines</a>
    and <a href="http://www.stat.duke.edu/~sayan/561/2019/optim.pdf">optimization notes</a> 
<ul>
<li> Optional: (video) Nate
  Otten <a href="https://www.youtube.com/watch?v=8-xkiGB28zc">Introduction
    to conjugate gradient</a> </li>
<li> Optional*: Andrew Stuart and Jochen
  Voss <a href="http://m.seehuhn.de/papers/numlinalg.pdf">Matrix
  analysis and algorithms pg. 75--83</a> 
</li>
<BR>
</ul>
</li>
<b> <a href="http://www.stat.duke.edu/~sayan/561/2019/Lab2_handout.pdf">Handout for Lab 2</a>  </b>
<BR>
<BR> 
<li> (Jan 31st) Gaussian process regression:<a href="http://www.stat.duke.edu/~sayan/561/2019/lec9.pdf"> Lecture</a> </li>
<ul>
<li> Optional: (video) Karl Rasmussen<a href="http://videolectures.net/mlss09uk_rasmussen_gp/"> Gaussian processes</a>
</li><li> Optional: (video) David
    MacKay <a href="http://videolectures.net/gpip06_mackay_gpb/">Gaussian
      Process Basics</a></li>
</li><li> Optional*:
  J.L. Doob <a href="http://www.stat.duke.edu/~sayan/561/2015/doob.pdf">The
  elementary Gaussian process</a></li>
</ul>
<BR>
</li>
<b> <a href="http://www.stat.duke.edu/~sayan/561/2019/Lab3_handout.pdf">Handout for Lab 3</a>  </b>
<BR>
<BR>
<li> (Feb 5th) Sparse regression: <a href="http://www.stat.duke.edu/~sayan/561/2019/lec10.pdf"> Lecture</a> </li>
<ul>
<li> Optional: (video) Daniela Witten and Robert
  Tibshirani<a href="https://www.youtube.com/watch?v=A5I1G1MfUmA"> The
  Lasso</a></li>
<li> Optional: (video) Trevor
  Hastie<a href="https://www.youtube.com/watch?v=BU2gjoLPfDc&feature=youtu.be">
    glmnet package</a></li>
</ul>
</li>
<BR>
<b> <a href="http://www.stat.duke.edu/~sayan/561/2019/HW2.pdf">HW
    2  </a> <a href="http://www.stat.duke.edu/~sayan/561/2019/HW2_geyser.csv">Dataset 1
    for HW
    2</a> <a href="http://www.stat.duke.edu/~sayan/561/2019/HW2_voting.csv">Dataset 2
    for HW 2</a> </b>
<BR>
<BR>
<li> (Feb 7th) Mixture models and latent space models I: 
<a href="http://www.stat.duke.edu/~sayan/561/2019/lec13.pdf">
  Lecture</a> </li> 
<ul>
<li>Optional: (video) Victor Lavrenko <a href="https://www.youtube.com/watch?v=REypj2sy_5U">Expectation maximization</a>
</li>
<li> Optional: (slides) David  Sontag <a href="http://cs.nyu.edu/~dsontag/courses/ml12/slides/lecture21.pdf">Expectation maximization</a></li>
<li> Optional*: Dempster, Laird,
  Rubin<a href="http://www.stat.missouri.edu/~dsun/9720/EM_JRSSB.pdf">
  Maximum Likelihood from Incomplete Data via the EM Algorithm</a>
</li>
</ul> 
<BR><li> (Feb 12th) Mixture models and latent space models II: 
<a href="http://www.stat.duke.edu/~sayan/561/2019/lec13.pdf">
  Lecture</a> </li> 
<ul>
<li>Optional: (video) Victor Lavrenko <a href="https://www.youtube.com/watch?v=REypj2sy_5U">Expectation maximization</a>
</li>
<li> Optional: (slides) David Sontag <a href="http://cs.nyu.edu/~dsontag/courses/ml12/slides/lecture21.pdf">Expectation maximization</a></li>
<li> Optional*: Dempster, Laird,
  Rubin<a href="http://www.stat.missouri.edu/~dsun/9720/EM_JRSSB.pdf">
  Maximum Likelihood from Incomplete Data via the EM Algorithm</a>
</li>
</ul> 
<BR>
</li>
</ul> 
<BR>
<b> <a href="http://www.stat.duke.edu/~sayan/561/2019/Lab4_handout.pdf">Handout for Lab 4</a>  </b>
<BR>
<BR>
<b> <a href="http://www.stat.duke.edu/~sayan/561/2019/PracMid.pdf">Practice midterm</a>  </b>
<BR>
<BR>
</li>
<li> (Feb 14th) Latent  Dirichlet Allocation I: <a href="http://www.stat.duke.edu/~sayan/561/2019/lec14.pdf">
  Lecture</a> </li> 
  <ul>
<li> Optional: (video) Dave
  Blei <a href="http://videolectures.net/mlss09uk_blei_tm/"> Topic models</a>
</li>
<li> Optional: (video) John Novembre <a href="https://www.youtube.com/watch?v=PMzw7YVsZAc"> Methods for the analysis of population structure and admixture</a>
</li>
<li> Optional: (slides) Dave Blei <a href="http://www.cs.columbia.edu/~blei/talks/Blei_ICML_2012.pdf">
  Probabilistic Topic Models</a>
</li>
</li>
<li> Optional: Pritchard, Stephens, Donnelly <a href="http://pritchardlab.stanford.edu/publications/structure.pdf">
  Inference of Population Structure Using Multilocus Genotype Data</a>
</li>
<li> Optional: Blei, Ng, Jordan <a href="https://www.cs.princeton.edu/~blei/papers/BleiNgJordan2003.pdf">
  Latent Dirichlet Allocation</a>
</li>
</ul>
  <BR>
</li>
<li> (Feb 19th) Latent  Dirichlet Allocation II: <a href="http://www.stat.duke.edu/~sayan/561/2019/lec14.pdf">
  Lecture</a> </li> 
  <ul>
<li> Optional: (video) Dave
  Blei <a href="http://videolectures.net/mlss09uk_blei_tm/"> Topic models</a>
</li>
<li> Optional: (video) John Novembre <a href="https://www.youtube.com/watch?v=PMzw7YVsZAc"> Methods for the analysis of population structure and admixture</a>
</li>
<li> Optional: (slides) Dave Blei <a href="http://www.cs.columbia.edu/~blei/talks/Blei_ICML_2012.pdf">
Probabilistic Topic Models</a>
</li>
<li> Optional: Pritchard, Stephens, Donnelly <a href="http://pritchardlab.stanford.edu/publications/structure.pdf">
  Inference of Population Structure Using Multilocus Genotype Data</a>
</li>
<li> Optional: Blei, Ng, Jordan <a href="https://www.cs.princeton.edu/~blei/papers/BleiNgJordan2003.pdf">
  Latent Dirichlet Allocation</a>
</li>
</ul>
  <BR>
<b>  (Feb 14-Feb 20) Take home midterm </b>
  <BR>
  <BR>
<b> <a href="http://www.stat.duke.edu/~sayan/561/2019/Lab5_handout.pdf">Handout for Lab 5</a>  </b>
<BR>
<BR>
<li> (Feb 21st) Markov chain Monte Carlo I: <a href="http://www.stat.duke.edu/~sayan/561/2019/lec15.pdf">
  Lecture</a> </li> 
  <ul>
<li> Optional: (video) Iain
  Murray <a href="http://videolectures.net/mlss09uk_murray_mcmc/"> MCMC</a>
</li>
<li> Optional: (slides) Iain  Murray <a href="http://homepages.inf.ed.ac.uk/imurray2/teaching/09mlss/slides.pdf"> MCMC</a>
</li>
<li> Optional: Casella, George <a href="https://stat.duke.edu/~scs/Courses/Stat376/Papers/Basic/CasellaGeorge1992.pdf">
  Explaining the Gibbs Sampler</a>
</li>
<li> Optional*: Levin, Peres, Wilmer <a href="http://pages.uoregon.edu/dlevin/MARKOV/markovmixing.pdf">Markov
  Chains and Mixing times</a>
</li>
<li> Optional*: Metropolis, Rosenbluth, Rosenbluth, Teller, Teller <a href="https://ssl.cs.dartmouth.edu/~gevorg/89/13W/Metropolis_MC.pdf">Equation of State Calculations by Fast Computing Machines </a>
</li>
</ul>
<BR><li> (Feb 26th) Markov chain Monte Carlo II: <a href="http://www.stat.duke.edu/~sayan/561/2019/lec15.pdf">
  Lecture</a> </li> 
  <ul>
<li> Optional: (video) Iain
  Murray <a href="http://videolectures.net/mlss09uk_murray_mcmc/"> MCMC</a>
</li>
<li> Optional: (slides) Iain  Murray <a href="http://homepages.inf.ed.ac.uk/imurray2/teaching/09mlss/slides.pdf"> MCMC</a>
</li>
<li> Optional: Casella, George <a href="https://stat.duke.edu/~scs/Courses/Stat376/Papers/Basic/CasellaGeorge1992.pdf">
  Explaining the Gibbs Sampler</a>
</li>
<li> Optional*: Levin, Peres, Wilmer <a href="http://pages.uoregon.edu/dlevin/MARKOV/markovmixing.pdf">Markov
  Chains and Mixing times</a>
</li>
<li> Optional*: Metropolis, Rosenbluth, Rosenbluth, Teller, Teller <a href="https://ssl.cs.dartmouth.edu/~gevorg/89/13W/Metropolis_MC.pdf">Equation of State Calculations by Fast Computing Machines </a>
</li>
  </ul>
    <BR>
<b> <a href="http://www.stat.duke.edu/~sayan/561/2019/Lab6_handout.pdf">Handout for Lab 6</a>  </b>
<BR>
<BR>
<li> (Feb 28th) Hidden Markov models
  II  <a href="http://www.stat.duke.edu/~sayan/561/2019/lec16.pdf">
  Lecture </a> and  <a href="http://www.stat.duke.edu/~sayan/561/2020/HMM-notes-scanned.pdf">
  more lecture notes</a> </li> </li>
<ul>
<li> Optional: (video) Nando de Freitas <a href="https://www.youtube.com/watch?v=jY2E6ExLxaw"> HMMs</a>
</li>
<li> Optional: (slides) Eric Xing <a href="https://www.cs.cmu.edu/~epxing/Class/10701-08s/recitation/em-hmm.pdf"> HMMs</a>
</li>
<li> Optional: Rabiner <a href="http://www.ece.ucsb.edu/Faculty/Rabiner/ece259/Reprints/tutorial%20on%20hmm%20and%20applications.pdf">A
  Tutorial on Hidden Markov Models and. Selected Applications in
  Speech Recognition. </a>
</li>
</ul>
<BR>
<li> (March 4th) Dimension reduction and embeddings I<a href="http://www.stat.duke.edu/~sayan/561/2019/lec17.pdf">
  Lecture</a> </li> 
  <ul>
<li> Optional: (video) Nando de Freitas <a href="https://www.youtube.com/watch?v=jY2E6ExLxaw"> HMMs</a>
</li>
<li> Optional: (slides) Eric Xing <a href="https://www.cs.cmu.edu/~epxing/Class/10701-08s/recitation/em-hmm.pdf"> HMMs</a>
</li>
<li> Optional: Rabiner <a href="http://www.ece.ucsb.edu/Faculty/Rabiner/ece259/Reprints/tutorial%20on%20hmm%20and%20applications.pdf">A
  Tutorial on Hidden Markov Models and. Selected Applications in
  Speech Recognition. </a>
</li>
</ul>
<BR>
<li> (March 4th) Dimension reduction and embeddings I<a href="http://www.stat.duke.edu/~sayan/561/2019/lec17.pdf">
  Lecture</a> </li> 
<ul>
  <li> Optional: (video) Juan
  Orduz <a href="https://www.youtube.com/watch?v=U31TIICsHiA">Laplacian eigenmaps</a>
  </li>
  <li> Optional: (video) Yann
  LeCun <a href="https://www.youtube.com/watch?v=UGPT64wo7lU">Graph emebddings</a>
      </li>
    <li> Optional: (video) Laurens van der Maaten <a href="https://www.youtube.com/watch?v=RJVL80Gg3lA">t-SNE</a>
  </li>
    <li> Optional: (video) Konstantinos
     Perifanos <a href="https://www.youtube.com/watch?v=6xPnEh_tJEc">Word embeddings</a>
    </li>
  <li> Optional: Dasgupta and Gupta
  <a href="http://www.cs.cmu.edu/~anupamg/papers/jl.pdf"> Johnson
  Lindenstrauss Lemma</a>
        </li>    
</ul>
<BR><li> (March 6th) Dimension reduction and embeddings II<a href="http://www.stat.duke.edu/~sayan/561/2019/lec17.pdf">
  Lecture</a> </li> 
<ul>
  <li> Optional: (video) Juan
  Orduz <a href="https://www.youtube.com/watch?v=U31TIICsHiA">Laplacian eigenmaps</a>
  </li>
  <li> Optional: (video) Yann
  LeCun <a href="https://www.youtube.com/watch?v=UGPT64wo7lU">Graph emebddings</a>
      </li>
    <li> Optional: (video) Laurens van der Maaten <a href="https://www.youtube.com/watch?v=RJVL80Gg3lA">t-SNE</a>
  </li>
    <li> Optional: (video) Konstantinos
     Perifanos <a href="https://www.youtube.com/watch?v=6xPnEh_tJEc">Word embeddings</a>
    </li>
  <li> Optional: Dasgupta and Gupta
  <a href="http://www.cs.cmu.edu/~anupamg/papers/jl.pdf"> Johnson
  Lindenstrauss Lemma</a>
        </li>    
</ul>
  
  <BR>  
<b>  The following are zoom videos and material for the now remote class, I will probably update every two days</b>  
<BR>
  
  <BR><li> (March 25th) Statistical learning theory:
  <a href="http://www.stat.duke.edu/~sayan/561/2019/lec12.pdf">
    Lecture  </a>
     <a href="https://urldefense.com/v3/__https://duke.zoom.us/rec/share/6MV6aL7fpkZOeJHN8X7_W_YQGYLIX6a82iBL_6YPyUj9bHhzRnL8LcAI9lpU2e7z__;!!OToaGQ!7q8xOrPWtwr-unS8yZLiXf-fZ6z9hLwT-GQCJyDwM4mvpe7VN4kx1z74wT19JMrAig$">
     Zoom lecture  </a>  <a href="http://www2.stat.duke.edu/~sayan/561/2020/SLT.pdf">
      Slides</a>  </li>
  <ul>
    <li> Optional: (video) Leon Bottou and Vladimir
      Vapnik <a href="http://videolectures.net/mmdss07_bottou_fslt/">Foundations
        of Statistical Learning</a>
    </li>
  </li>
    <li> Optional: Vladimir Vapnik and
        Ya. Chervonenkis
      <a href="https://courses.engr.illinois.edu/ece544na/fa2014/v\
         apnik71.pdf">On
          the Uniform Convegence of Relative Frequencies of Events to
        their
        Probabilities</a></li>
    <li> Optional*: Michel
      Talagrand <a href="http://www.stat.duke.edu/~sayan/561/2015/talagrand.pdf">The
        Glivenko-Cantelli Problem</a></li>
  </ul>
 <BR> 
  <BR> 
<li> (April 1st) Neural networks I <a href="http://www.stat.duke.edu/~sayan/561/2019/lec18.pdf">
    Lecture</a> <a href="https://duke.zoom.us/rec/share/--lYKuDb2mZJa7Pmxn7tdaUAEIroeaa80CMXr_FczhkGWeuCYemg7D1W-jIwLY21">
     Zoom lecture  </a> </li>
<ul>
   <li>
   Trivedi and Kondor <a href="https://ttic.uchicago.edu/~shubhendu/Pages/Files/Lecture3_pauses.pdf">Backprop
       1</a> </li>
  <li>
   Trivedi and Kondor <a href="https://ttic.uchicago.edu/~shubhendu/Pages/Files/Lecture4_pauses.pdf">Backprop
      2</a> </li>
<li>   Trivedi and
      Kondor <a href="https://ttic.uchicago.edu/~shubhendu/Pages/Files/Lecture7_pauses.pdf">Convolutional
    nets</a> </li>
<li>
   Le Cun et
   al <a href="https://papers.nips.cc/paper/293-handwritten-digit-recognition-with-a-back-propagation-network.pdf">LeNet</a> </li>
<li>
Krizhevsky et
al <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">AlexNet</a> </li>
<li>
Leung <a href="https://www.youtube.com/watch?v=bNb2fEVKeEo">Lecture
    on convolutional nets</a> </li>
</ul>
<BR>   
   
  <li> (April 3rd) Neural networks I <a href="http://www.stat.duke.edu/~sayan/561/2019/lec18.pdf">
    Lecture</a> <a href="https://duke.zoom.us/rec/share/--lYKuDb2mZJa7Pmxn7tdaUAEIroeaa80CMXr_FczhkGWeuCYemg7D1W-jIwLY21">
     Zoom lecture  </a> </li>
<ul>
   <li>
   Trivedi and Kondor <a href="https://ttic.uchicago.edu/~shubhendu/Pages/Files/Lecture3_pauses.pdf">Backprop
       1</a> </li>
  <li>
   Trivedi and Kondor <a href="https://ttic.uchicago.edu/~shubhendu/Pages/Files/Lecture4_pauses.pdf">Backprop
      2</a> </li>
<li>   Trivedi and
      Kondor <a href="https://ttic.uchicago.edu/~shubhendu/Pages/Files/Lecture7_pauses.pdf">Convolutional
    nets</a> </li>
<li>
   Le Cun et
   al <a href="https://papers.nips.cc/paper/293-handwritten-digit-recognition-with-a-back-propagation-network.pdf">LeNet</a> </li>
<li>
Krizhevsky et
al <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">AlexNet</a> </li>
<li>
Leung <a href="https://www.youtube.com/watch?v=bNb2fEVKeEo">Lecture
    on convolutional nets</a> </li>
</ul>
<BR>   
  
 <li> (April 8st) Optimization I <a href="https://duke.zoom.us/rec/share/zs0pBe_f8XhIY9brxUT9QJAbGavUaaa82iAW-PMMy29pP9Al2LbGBhNhpG1KrME">
  Zoom video</a> <a href="https://www2.stat.duke.edu/~sayan/561/2020/opt.pdf">
  notes</a> </li> </li>
<ul>
 <li> Optional: (video) Sham Kakade
       <a href="https://www.youtube.com/watch?v=_UFGB2MBo4o/">Accelerating Stochastic Gradient Descent</a>
    </li>
    <li> Optional: Leon Bottou
      <a href="https://leon.bottou.org/slides/largescale/lstut.pdf"> Large Scale Machine Learning</a></li>
    <li> Optional: John
    Canny<a href="https://bcourses.berkeley.edu/courses/1453965/files/69573148/download?verifier=XngOkhZvNZpG1OedI7quhUn10HEpW4kf4Gu0HBqQf">
        Stochastic Gradient Descent, Slide 49 is great</a></li>
  <li> Optional: Trevedi and Kondor<a href="https://ttic.uchicago.edu/~shubhendu/Pages/Files/Lecture6_pauses.pdf">
        Stochastic Gradient Descent</a></li>
</ul> 
  
  <BR>
 <li> (April 21-30) Final exam <a href="http://www.stat.duke.edu/~sayan/561/2020/Final2019.pdf"> Example final</a> </li>
  
  
<BR>
 <li> (May 2nd) Final project due </li>

  
  
<BR>  
<b>  Below was the order of lectures planned before we had to go remote</b>  
<BR>
  
  
<BR>
<li> (March 18th) Neural networks I <a href="http://www.stat.duke.edu/~sayan/561/2019/lec18.pdf">
    Lecture</a> </li>
<ul>
   <li>
   Trivedi and Kondor <a href="https://ttic.uchicago.edu/~shubhendu/Pages/Files/Lecture3_pauses.pdf">Backprop
       1</a> </li>
  <li>
   Trivedi and Kondor <a href="https://ttic.uchicago.edu/~shubhendu/Pages/Files/Lecture4_pauses.pdf">Backprop
      2</a> </li>
<li>   Trivedi and
      Kondor <a href="https://ttic.uchicago.edu/~shubhendu/Pages/Files/Lecture7_pauses.pdf">Convolutional
    nets</a> </li>
<li>
   Le Cun et
   al <a href="https://papers.nips.cc/paper/293-handwritten-digit-recognition-with-a-back-propagation-network.pdf">LeNet</a> </li>
<li>
Krizhevsky et
al <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">AlexNet</a> </li>
<li>
Leung <a href="https://www.youtube.com/watch?v=bNb2fEVKeEo">Lecture
    on convolutional nets</a> </li>
</ul>
<BR>
<li> (March 20th) Neural networks II <a href="http://www.stat.duke.edu/~sayan/561/2019/lec18.pdf">
    Lecture</a> </li>
<ul>
   <li>
   Trivedi and Kondor <a href="https://ttic.uchicago.edu/~shubhendu/Pages/Files/Lecture3_pauses.pdf">Backprop
       1</a> </li>
  <li>
   Trivedi and Kondor <a href="https://ttic.uchicago.edu/~shubhendu/Pages/Files/Lecture4_pauses.pdf">Backprop
      2</a> </li>
<li>   Trivedi and
      Kondor <a href="https://ttic.uchicago.edu/~shubhendu/Pages/Files/Lecture7_pauses.pdf">Convolutional
    nets</a> </li>
<li>
<li>
   Le Cun et
   al <a href="https://papers.nips.cc/paper/293-handwritten-digit-recognition-with-a-back-propagation-network.pdf">LeNet</a> </li>
<li>
Krizhevsky et
al <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">AlexNet</a> </li>
<li>
Leung <a href="https://www.youtube.com/watch?v=bNb2fEVKeEo">Lecture
    on convolutional nets</a> </li>
</ul>
<BR>
<li> (March 25th) Variational methods and Generative Adversarial Networks I<a href="http://www.stat.duke.edu/~sayan/561/2019/lec19.pdf">
  Lecture</a> </li> </li>
<ul>
</ul>
<BR>
<li> (March 27th) Variational methods and Generative Adversarial Networks II<a href="http://www.stat.duke.edu/~sayan/561/2019/lec20.pdf">
  Lecture</a> </li> </li>
<ul>
</ul>
<BR>
<li> (April 1st) Optimization I <a href="http://www.stat.duke.edu/~sayan/561/2019/lec20.pdf">
  Lecture</a> </li> </li>
<ul>
 <li> Optional: (video) Sham Kakade
       <a href="https://www.youtube.com/watch?v=_UFGB2MBo4o/">Accelerating Stochastic Gradient Descent</a>
    </li>
    <li> Optional: Leon Bottou
      <a href="https://leon.bottou.org/slides/largescale/lstut.pdf"> Large Scale Machine Learning</a></li>
    <li> Optional: John
    Canny<a href="https://bcourses.berkeley.edu/courses/1453965/files/69573148/download?verifier=XngOkhZvNZpG1OedI7quhUn10HEpW4kf4Gu0HBqQf">
        Stochastic Gradient Descent, Slide 49 is great</a></li>
  <li> Optional: Trevedi and Kondor<a href="https://ttic.uchicago.edu/~shubhendu/Pages/Files/Lecture6_pauses.pdf">
        Stochastic Gradient Descent</a></li>
</ul>
<BR><li> (April 3rd) Optimization II <a href="http://www.stat.duke.edu/~sayan/561/2019/lec20.pdf">
  Lecture</a> </li> </li>
  <ul>
 <li> Optional: (video) Sham Kakade
       <a href="https://www.youtube.com/watch?v=_UFGB2MBo4o/">Accelerating Stochastic Gradient Descent</a>
    </li>
    <li> Optional: Leon Bottou
      <a href="https://leon.bottou.org/slides/largescale/lstut.pdf"> Large Scale Machine Learning</a></li>
    <li> Optional: John
    Canny<a href="https://bcourses.berkeley.edu/courses/1453965/files/69573148/download?verifier=XngOkhZvNZpG1OedI7quhUn10HEpW4kf4Gu0HBqQf">
        Stochastic Gradient Descent, Slide 49 is great</a></li>
  <li> Optional: Trevedi and Kondor<a href="https://ttic.uchicago.edu/~shubhendu/Pages/Files/Lecture6_pauses.pdf">
        Stochastic Gradient Descent</a></li>
</ul>
 <BR> 
<li> (April 8th) Computational differentiation <a href="">
    Lecture</a> </li>
<ul>
 <li> Optional: Baydin, Pearlmutter, Radul, and Siskind
       <a href="https://arxiv.org/pdf/1502.05767.pdf">Automatic Differentiation</a>
 </li>
    <li> Optional: Maclaurin, Duvenaud, and Adams
      <a href="https://arxiv.org/abs/1502.03492"> Reversible learning with exact arithmetic</a></li>
    <li> Optional:
      Kathura <a href="https://towardsdatascience.com/getting-started-with-pytorch-part-1-understanding-how-automatic-differentiation-works-5008282073ec">Getting
        Started with PyTorch </a></li>
 <li> Optional:
      Altexsoft <a href="https://www.altexsoft.com/blog/datascience/choosing-an-open-source-machine-learning-framework-tensorflow-theano-torch-scikit-learn-caffe/">
      Machine Learning Libraries </a></li>
</ul>
<BR><li> (April 10th) Statistical learning theory I:
  <a href="http://www.stat.duke.edu/~sayan/561/2019/lec12.pdf">
    Lecture</a>
  <ul>
    <li> Optional: (video) Leon Bottou and Vladimir
      Vapnik <a href="http://videolectures.net/mmdss07_bottou_fslt/">Foundations
        of Statistical Learning</a>
    </li>
  </li>
    <li> Optional: Vladimir Vapnik and
        Ya. Chervonenkis
      <a href="https://courses.engr.illinois.edu/ece544na/fa2014/v\
         apnik71.pdf">On
          the Uniform Convegence of Relative Frequencies of Events to
        their
        Probabilities</a></li>
    <li> Optional*: Michel
      Talagrand <a href="http://www.stat.duke.edu/~sayan/561/2015/talagrand.pdf">The
        Glivenko-Cantelli Problem</a></li>
  </ul>
 <BR> 
<li> (April 15th) Computational differentiation <a href="">
    Lecture</a> </li>
<BR>




 
</body></html>
